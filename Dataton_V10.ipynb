{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datatón Bancolombia 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libreria | objetivo\n",
    "\n",
    "#procesamiento\n",
    "import pandas as pd   #manejo de dataframes\n",
    "from dask import dataframe as dd #pandas + numpy + sklearn con paralelismo\n",
    "\n",
    "#matematicas, estadistica y graficas\n",
    "import numpy as np #numpy para funciones matematicas\n",
    "import matplotlib.pyplot as plt #para graficar\n",
    "import seaborn as sns #para graficar\n",
    "import matplotlib.ticker as mtick #para ajustar estilo de ejes \n",
    "from scipy.stats import median_absolute_deviation #para calcular la MAD\n",
    "from sklearn.model_selection import train_test_split #para el random split de los datos\n",
    "from sklearn.preprocessing import StandardScaler  #para el scaler de las variables\n",
    "from sklearn.decomposition import PCA #para hacer componentes principales\n",
    "\n",
    "#Modelos\n",
    "\n",
    "#supervizados\n",
    "from sklearn.ensemble import RandomForestRegressor #para hacer un RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge #regresion ridge\n",
    "from sklearn.svm import SVR #regresion SVM\n",
    "from sklearn.neighbors import KNeighborsRegressor #regresion KNN\n",
    "from sklearn.ensemble import GradientBoostingRegressor #regresion Gradient Boosting\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "#no supervizados\n",
    "from sklearn.cluster import KMeans #k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración del notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy = True #para que el notebook autocomplete\n",
    "plt.style.use('Solarize_Light2') #ajustar estilo de matplotlib  Solarize_Light2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuck_comas(df):  \n",
    "    \n",
    "    '''Soluciona el problema con las profesiones que en su valor tienen comas, haciendo que la lectura del csv falle.\n",
    "    Parte de que a la lectura del DataFrame se le añadieron dos columnas ficticias (col1 y col2) para manejar el desplazamiento.'''\n",
    "    \n",
    "    df.iloc[df.loc[df.col2.notnull()].index, 7] = df.iloc[df.loc[df.col2.notnull()].index, 7] + df.iloc[df.loc[df.col2.notnull()].index, 8] + df.iloc[df.loc[df.col2.notnull()].index, 9] #la columna 7 es concatenada con valores que se encuentran hasta dos columnas mas adelante\n",
    "    df.iloc[df.loc[df.col2.notnull()].index, 8:65] = df.iloc[df.loc[df.col2.notnull()].index, 10:67].to_numpy() #las columnas posteriores a la 7 son llenadas con los valores con los valores que se encuentran desplazadas dos veces\n",
    "    \n",
    "    df.iloc[df.loc[df.col2.notnull()].index, 65:67] = float('nan')  #las columnas dummies col1 y col2 se llenan de nan\n",
    "    df.drop(columns = ['col2'], inplace = True) #se elimina la columna col2\n",
    "    \n",
    "    df.iloc[df.loc[df.col1.notnull()].index, 7] = df.iloc[df.loc[df.col1.notnull()].index, 7] + df.iloc[df.loc[df.col1.notnull()].index, 8] #la columna 7 es concatenada con un valor desplazado una columna para solucionar lo de las profesiones que solo desplazaron los datos una vez\n",
    "    df.iloc[df.loc[df.col1.notnull()].index, 8:65] = df.iloc[df.loc[df.col1.notnull()].index, 9:66].to_numpy() #las columnas posteriores a la 7 se llenan con los datos que solo se desplazaron una vez\n",
    "    df.drop(columns = ['col1'], inplace = True) #se elimina la columna col1\n",
    "    \n",
    "    return df\n",
    "\n",
    "def dtype_errors(df):\n",
    "    \n",
    "    '''Imprime los valores unicos de todas las columnas de un DataFrame, posibilitanto encontrar valores que se encuentres desplazados'''\n",
    "    \n",
    "    type_errors = {df.columns[i]: df[df.columns[i]].unique() for i in range(0, len(df.columns))}  #trae los valores unicos de cada columna\n",
    "    \n",
    "    print(type_errors)\n",
    "    \n",
    "def t_ult_actual_y_covid(df):\n",
    "    \n",
    "    '''Reemplaza la columna ult_actual del dataframe original por la columna t_ult_actual, \n",
    "    que corresponde a la cantidad de meses entre periodo y ult_actual. Adicionalmente, elimina los registros donde la \n",
    "    variable periodo y ult_actual son nulos, ademas de eliminar los que no continen el str(2) porque entonces no \n",
    "    podrian ser fechas del siglo XXI, ademas de filtrar por la cantidad de caracteres.\n",
    "    \n",
    "    A su vez, esta funcion verifica el valor de la columna periodo para clasificar por precovid y postcovid'''\n",
    "    \n",
    "    df = df[df.periodo.notnull()] #elimina los registros donde la variable periodo y ult_actual son nulos\n",
    "    df = df[df.ult_actual.notnull()] \n",
    "\n",
    "    df = df[df.periodo.str.contains(\"2\")]   #elimina los que no continen el str(2) porque entonces no podrian ser fechas del siglo XXI\n",
    "    df = df[df.ult_actual.str.contains(\"2\")] \n",
    "\n",
    "    df = df[df['ult_actual'].map(len) == 8]  #eliminamos todos los registros de esta variable que no tienen 8 de longitud\n",
    "    df = df[df['periodo'].map(len) == 6]   ##eliminamos todos los registros de esta variable que no tienen 6 de longitud\n",
    "\n",
    "    periodo = [[int(x[:4])*12, int(x[4:])] for x in df.periodo] #lista de listas del periodo partido en [año, mes] donde el año se ha convertido a meses\n",
    "    ult_actual = [[int(x[:4])*12, int(x[4:6])] for x in df.ult_actual] #lista de listas del ult_actual partido en [año, mes] donde el año se ha convertido a meses\n",
    "    a = [periodo[i][0] - ult_actual[i][0] + (periodo[i][1] - ult_actual[i][1]) for i in range(0,len(periodo))] #equivalente a un np.sum(dataframe, axis =1) para sumar los años convertidos y los meses y obtener el tiempo total\n",
    "    \n",
    "    #los sgtes pasos hicieron parte del codigo, pero numpy tiene muchos conflictos con dask, entonces lo que hacen las \n",
    "    #lineas siguientes ahora es hecho sin numpy por las tres lineas anteriores.\n",
    "    #a = np.array(periodo) - np.array(ult_actual) #elementwise diferencia\n",
    "    #a[:, 0] = a[:, 0] * 12 #conversion de años a meses\n",
    "    #a = np.sum(a, axis = 1) #suma por axis = 1, para que sea por columna\n",
    "    \n",
    "    df['ult_actual'] = a #redefinicion de ult_actual\n",
    "    df = df.rename(columns = {'ult_actual': 't_ult_actual'}) #cambio de nombre\n",
    "    \n",
    "    #lo siguiente toma el valor de la columna periodo para clasificar por precovid y postcovid\n",
    "    #era una funcion aparte de t_ult_actual pero como obvio los filtros que estan al inicio de t_ult_actual\n",
    "    #no se han computado cuando se declara la funcion covid en el map_partitions, entonces se tienen los \n",
    "    #mismos errores de preprocesamiento, es decir, campos que no se pueden convertir a enteros, etc.\n",
    "    #para ahorrar CPU y RAM se prefiere implementarla aca mismo\n",
    "     \n",
    "    df['mes'] = [int(i[-2:]) for i in df.periodo] #para capturar el numero del mes\n",
    "    df.mes = df.mes.replace(to_replace = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], value = ['enero', 'febrero', 'marzo', \n",
    "                                                                                          'abril', 'mayo', 'junio', \n",
    "                                                                                          'julio', 'agosto', 'septiembre', \n",
    "                                                                                          'octubre', 'noviembre', 'diciembre']) #para poner el nombre del mes\n",
    "    \n",
    "    df['periodo'] = [1 if int(i) >= 202003 else 0 for i in df.periodo] #redefinicion de periodo para tener el flag de covid\n",
    "    df = df.rename(columns = {'periodo': 'covid'}) #cambio de nombre    \n",
    "    \n",
    "    return df\n",
    "\n",
    "def edad_to_float(df):\n",
    "    \n",
    "    '''convierte la columna edad a un flotante y reemplaza \\\\N por la mediana.\n",
    "    Tiene sentido imputar porque los NA son cerca de 0.03%'''\n",
    "    \n",
    "    df.edad = df.edad.replace(to_replace = ['\\\\N','foo'], value = [float('nan'), float('nan')]) #reemplaza los '\\\\N' de edad como nan, tambien el 'foo'\n",
    "    df['edad'] = [float(i) for i in df['edad']]  #convierte todo a flotante\n",
    "    edad_median = df.edad.median()  #calcula la mediana \n",
    "    df['edad'] = df.edad.fillna(edad_median)  #reemplaza la mediana\n",
    "    \n",
    "    return df\n",
    "\n",
    "def genero_limpio(df):\n",
    "    \n",
    "    '''imputa el valor ' ' de la columna genero por la moda, tiene sentido imputar asi porque hay pocos ' ' '''\n",
    "    \n",
    "    df.genero = df.genero.replace(to_replace = ' ', value = float('nan')) #reemplaza los ' ' de genero como nan\n",
    "    genero_mode = df.genero.mode().iloc[0]  #calcula la moda \n",
    "    df['genero'] = df.genero.fillna(genero_mode)  #reemplaza la moda\n",
    "    \n",
    "    return df\n",
    "\n",
    "def estado_civil_limpio(df):\n",
    "    \n",
    "    '''imputa el valor de \\\\N de la columna estado_civil por la moda \n",
    "    tiene sentido porque son pocos los que no reportan'''\n",
    "    \n",
    "    df.estado_civil = df.estado_civil.replace(to_replace = '\\\\N', value = float('nan')) #reemplaza los '\\\\N' de estado civil como nan\n",
    "    estado_civil_mode = df.estado_civil.mode().iloc[0]  #calcula la moda \n",
    "    df['estado_civil'] = df.estado_civil.fillna(estado_civil_mode)  #reemplaza la moda\n",
    "    \n",
    "    return df\n",
    "\n",
    "def nivel_academico_limpio(df):\n",
    "    \n",
    "    '''Reemplaza el valor NO INFORMA por SIN INFORMACION, asumimos que no tiene sentido imputador en esta variable'''\n",
    "    \n",
    "    df.nivel_academico = df.nivel_academico.replace(to_replace = 'NO INFORMA', value = 'SIN INFORMACION') #reemplaza los 'NO INFORMA' de genero como 'SIN INFORMACION'\n",
    "    \n",
    "    return df\n",
    "\n",
    "def profesion_limpia(df):\n",
    "    \n",
    "    '''Elimina la variable profesion. No se deja en el modelo porque la matriz seria muy dispersa, sin embargo, \n",
    "    luego podrian agruparse las profesiones por categorias similares (si se quiere)'''\n",
    "    \n",
    "    df.drop('profesion', inplace = True, axis=1) #elimina la columna profesion\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    \n",
    "def ocupacion_limpia(df):\n",
    "    \n",
    "    '''Imputa en ocupacion y corrige los faltantes'''\n",
    "    \n",
    "    df.ocupacion = df.ocupacion.replace(to_replace = ['Profesional Independiente' , '\\\\N', 'Jubilado', 'Vacío'], \n",
    "                                        value = ['Independiente', 'Sin Ocupacion Asignada', 'Pensionado', 'Sin Ocupacion Asignada'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def tipo_vivienda_limpia(df):\n",
    "\n",
    "    df.tipo_vivienda = df.tipo_vivienda.replace(to_replace = ['\\\\N'], \n",
    "                                    value = ['NO INFORMA'])\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def categoria_limpio(df):\n",
    "    \n",
    "    '''imputa el valor de \\\\N de la columna categoria por la moda SIN EMBARGO, SERIE INTERESANTE ENTRENAR UN CLASIFICADOR\n",
    "    TIPO REGRESION LOGISTICA PARA SU CALCULO'''\n",
    "    \n",
    "    '''TAMBIEN SERIE INTERESANTE VER SI LA MODA SE REEMPLAZA POR PARTICION O POR TODA LA BASE DE DATOS'''\n",
    "    \n",
    "    df.categoria = df.categoria.replace(to_replace = '\\\\N', value = float('nan')) #reemplaza los '\\\\N' de categoria como nan\n",
    "    categoria_mode = df.categoria.mode().iloc[0]  #calcula la moda \n",
    "    df['categoria'] = df.categoria.fillna(categoria_mode)  #reemplaza la moda\n",
    "    \n",
    "    return df\n",
    "\n",
    "def ind_binarios_riesgo(df):\n",
    "    \n",
    "    '''Imputa los valores \"\\\\N\" por \"N\" puesto que si no hay informacion lo mas normal seria que no tenga (el banco \n",
    "    puede que no sepa si no tienes moras o carteras cast. pero si sabe cuando si).\n",
    "    luego convierte los \"S\" en 1 y los \"N\" en 0.\n",
    "    Aplica para: ind_mora_vigente, cartera_castigada'''\n",
    "\n",
    "    df.ind_mora_vigente = df.ind_mora_vigente.replace(to_replace = ['\\\\N','N','foo','S'], \n",
    "                                    value = ['0','0','0','1'])\n",
    "    df.cartera_castigada = df.cartera_castigada.replace(to_replace = ['\\\\N','N','foo','S'], \n",
    "                                    value = ['0','0','0','1'])\n",
    "    df.ind_mora_vigente = pd.to_numeric(df.ind_mora_vigente)\n",
    "    df.cartera_castigada = pd.to_numeric(df.cartera_castigada)\n",
    "\n",
    "    return df\n",
    "\n",
    "def rechazo_credito_limpio(df):\n",
    "    \n",
    "    '''Elimina la variable rechazo_credito porque el 99% son datos faltantes. Antes del análisis de nulos esta funcion hacia \n",
    "    lo sgte: \n",
    "    \n",
    "        Imputa los valores \"\\\\N\" por \"No Rechazos\" puesto que al tratarse de un flag de rechazo si no\n",
    "        aparece valor es por que no ha tenido rechazos\n",
    "        \n",
    "            df.rechazo_credito = df.rechazo_credito.replace(to_replace = ['\\\\N'], \n",
    "                                    value = ['No Rechazos'])\n",
    "    '''\n",
    "    \n",
    "    df = df.drop(columns = ['rechazo_credito'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def manejo_de_moras(df):\n",
    "    \n",
    "    '''Vamos a atacar las moras en conjunto bajo la misma hipotesis de los indicadores de riesgo si no hay info\n",
    "    es que no hay, el banco siempre sabe cuando si (sino para que putas pagaria datacredito). Igualmente para blindarnos\n",
    "    revisamos si tiene flag de moras de mas de 30, 60 o 90 en los ultimos 12 meses en alguno de los valores vacios de\n",
    "    mora max y no fue el caso. Ademas tambien comprobamos que donde hay valores de mora max, hay reportes para los indicadores\n",
    "    de moras de x dias correspondientes'''\n",
    "    \n",
    "    #no vi el foo en las primeras 8 particiones pero como me sacó error entonces se los meti a todos por silas\n",
    "    df.mora_max = df.mora_max.replace(to_replace = ['\\\\N','foo', float('nan')], \n",
    "                                    value = ['0','0', '0'])\n",
    "    \n",
    "    df.cant_moras_30_ult_12_meses = df.cant_moras_30_ult_12_meses.replace(to_replace = ['\\\\N','foo', float('nan')], \n",
    "                                    value = ['0','0', '0'])\n",
    "    \n",
    "    df.cant_moras_60_ult_12_meses = df.cant_moras_60_ult_12_meses.replace(to_replace = ['\\\\N','foo', float('nan')], \n",
    "                                    value = ['0','0', '0'])\n",
    "    \n",
    "    df.cant_moras_90_ult_12_meses = df.cant_moras_90_ult_12_meses.replace(to_replace = ['\\\\N','foo', float('nan')], \n",
    "                                    value = ['0','0', '0'])\n",
    "    \n",
    "    #de una vez convirtamos todo esto en numeros para irnos acercando a los tipos de datos ideales\n",
    "    df.mora_max = pd.to_numeric(df.mora_max)\n",
    "    df.cant_moras_30_ult_12_meses = pd.to_numeric(df.cant_moras_30_ult_12_meses)\n",
    "    df.cant_moras_60_ult_12_meses = pd.to_numeric(df.cant_moras_60_ult_12_meses)\n",
    "    df.cant_moras_90_ult_12_meses = pd.to_numeric(df.cant_moras_90_ult_12_meses)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def limpieza_TdC(df):\n",
    "    \n",
    "    '''Apriori no parece requerir imputacion mas alla de los foo, que corresponden a falta de info y por consiguiente \n",
    "    deberian ser 0. De cualquier manera la mediana de estos valores corresponde a 0 pues 60% de estos datos ya estan en 0 \n",
    "    y el foo es un valor ficticio explicado abajo'''\n",
    "    \n",
    "    df.cupo_total_tc = df.cupo_total_tc.replace(to_replace = ['foo', float('nan')], \n",
    "                                    value = ['0', '0'])\n",
    "    df.cuota_tc_bancolombia = df.cuota_tc_bancolombia.replace(to_replace = ['foo', float('nan')], \n",
    "                                    value = ['0', '0'])\n",
    "    \n",
    "    df.cupo_total_tc = pd.to_numeric(df.cupo_total_tc)\n",
    "    df.cuota_tc_bancolombia = pd.to_numeric(df.cuota_tc_bancolombia)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def limpieza_obligaciones_cred(df):\n",
    "    \n",
    "    '''Dado que solo hay \"\\\\N\" y \"X\" los primeros se reemplazaran por 0 y los segundos por 1 en las columnas\n",
    "    de tiene_crediagil y tiene_consumo'''\n",
    "    \n",
    "    df.tiene_consumo = df.tiene_consumo.replace(to_replace = ['\\\\N', 'X', 'foo',float('nan')], \n",
    "                                    value = [0, 1, 0, 0])\n",
    "    df.tiene_crediagil = df.tiene_crediagil.replace(to_replace = ['\\\\N', 'X', 'foo', float('nan')], \n",
    "                                    value = [0, 1, 0, 0])\n",
    "    \n",
    "    df.tiene_consumo = pd.to_numeric(df.tiene_consumo)\n",
    "    df.tiene_crediagil = pd.to_numeric(df.tiene_crediagil)\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "def limpieza_cuentas(df):\n",
    "    \n",
    "    '''Para el numero de cuentas, cuentas embargas y actuvas,\n",
    "    parece seguro imputar un valor de 0 a lo que no se tenga info. ademas se comprobó que estos \\\\N son paralelos'''\n",
    "    \n",
    "    df.nro_tot_cuentas = df.nro_tot_cuentas.replace(to_replace = ['\\\\N', 'foo', float('nan')], \n",
    "                                    value = ['0', '0', '0'])\n",
    "    df.ctas_activas = df.ctas_activas.replace(to_replace = ['\\\\N', 'foo', float('nan')], \n",
    "                                    value = ['0', '0', '0'])\n",
    "    df.ctas_embargadas = df.ctas_embargadas.replace(to_replace = ['\\\\N', 'foo', float('nan')], \n",
    "                                    value = ['0', '0', '0'])\n",
    "    \n",
    "    df.nro_tot_cuentas = pd.to_numeric(df.nro_tot_cuentas)\n",
    "    df.ctas_activas = pd.to_numeric(df.ctas_activas)\n",
    "    df.ctas_embargadas = pd.to_numeric(df.ctas_embargadas)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def limpieza_fopep(df):\n",
    "    \n",
    "    '''Vamos a reemplazar el foo y el \\\\N  por 0 y la X por 1.'''\n",
    "    \n",
    "    df.pension_fopep = df.pension_fopep.replace(to_replace = ['\\\\N', 'foo', 'X', float('nan')], \n",
    "                                    value = ['0', '0', \"1\", '0'])\n",
    "    df.pension_fopep = pd.to_numeric(df.pension_fopep)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def limpieza_hipotecario(df):\n",
    "    \n",
    "    '''Vamos a limpiar lo concerniente a creditos hipotecarios.\n",
    "    Que en todo caso seria para cuota_cred_hipot, tiene_cred_hipo_1, tiene_cred_hipo_2'''\n",
    "    \n",
    "    df.tiene_cred_hipo_1 = df.tiene_cred_hipo_1.replace(to_replace = [float('nan'),'\\\\N', 'foo', 'X'], \n",
    "                                                value = ['0', '0', '0', \"1\"])\n",
    "    df.tiene_cred_hipo_2 = df.tiene_cred_hipo_2.replace(to_replace = [float('nan'),'\\\\N', 'foo', 'X'], \n",
    "                                            value = ['0', '0', '0', \"1\"])\n",
    "    df.cuota_cred_hipot = df.cuota_cred_hipot.replace(to_replace = ['\\\\N', 'foo', float('nan')], \n",
    "                                            value = ['0', '0', '0'])\n",
    "    \n",
    "    df.cuota_cred_hipot = pd.to_numeric(df.cuota_cred_hipot)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def limpieza_mediana_nom3(df):\n",
    "    \n",
    "    '''Esta funcion transforma los datos de mediana_nom3 en un float. Imputa los foo por la mediana'''\n",
    "    \n",
    "    df.mediana_nom3 = df.mediana_nom3.replace(to_replace = ['foo'], value = [float('nan')])\n",
    "    df['mediana_nom3'] = [float(i) for i in df['mediana_nom3']]  #convierte todo a flotante\n",
    "    mediana_nom3_median = df.mediana_nom3.median()  #calcula la mediana \n",
    "    df['mediana_nom3'] = df.mediana_nom3.fillna(mediana_nom3_median) \n",
    "    \n",
    "    return df\n",
    "\n",
    "def limpieza_mediana_pen3(df):\n",
    "    \n",
    "    '''Esta funcion transforma los datos de mediana_pen3 en un float. Imputa los foo  por la mediana'''\n",
    "    \n",
    "    df.mediana_pen3 = df.mediana_pen3.replace(to_replace = ['foo'], value = [float('nan')])\n",
    "    df['mediana_pen3'] = [float(i) for i in df['mediana_pen3']]  #convierte todo a flotante\n",
    "    mediana_pen3_median = df.mediana_pen3.median()  #calcula la mediana \n",
    "    df['mediana_pen3'] = df.mediana_pen3.fillna(mediana_pen3_median) \n",
    "    \n",
    "    return df\n",
    "\n",
    "def limpieza_ingreso_nompen(df):\n",
    "    \n",
    "    '''Esta funcion transforma los datos de ingreso_nompen en un float. Imputa los foo  por la mediana'''\n",
    "    \n",
    "    df.ingreso_nompen = df.ingreso_nompen.replace(to_replace = ['foo'], value = [float('nan')])\n",
    "    df['ingreso_nompen'] = [float(i) for i in df['ingreso_nompen']]  #convierte todo a flotante\n",
    "    ingreso_nompen_median = df.ingreso_nompen.median()  #calcula la mediana \n",
    "    df['ingreso_nompen'] = df.ingreso_nompen.fillna(ingreso_nompen_median) \n",
    "    \n",
    "    return df\n",
    "\n",
    "def limpieza_cat_ingreso(df):\n",
    "    \n",
    "    '''Esta funcion transforma los datos de ingreso_nompen en un float. Imputa los foo  por la mediana'''\n",
    "    \n",
    "    df.cat_ingreso = df.cat_ingreso.replace(to_replace = ['\\\\N', float('nan')], value = ['Sin informacion', 'Sin informacion'])\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "def limpeza_al_estilo_datacredito(df):\n",
    "    \n",
    "    '''Funcion que toma todas las variables desde ingreso final hasta el ind y aprovecha que todas son numéricas para hacerles\n",
    "    el mismo tratamiento: reemplazar foo's y \\\\N's por CEROS (ASUMUENDO QUE DATACREDITO SI TIENE ESA INFORMACION) \n",
    "    Y QUE SI APARECE \\\\NA ES PORQUE ES CERO.\n",
    "    Nos soplamos cuota_tc_mdo porque es combinacion lineal de cuota_tarjeta_de_credito y cuota_tc_bancolombia'''\n",
    "    \n",
    "    df = df.drop(columns = ['cuota_tc_mdo']) \n",
    "\n",
    "    variables_double = ['cant_mora_30_tdc_ult_3m_sf', 'cant_mora_30_consum_ult_3m_sf',\n",
    "   'cuota_de_vivienda', 'cuota_de_consumo', 'cuota_rotativos',\n",
    "   'cuota_tarjeta_de_credito', 'cuota_de_sector_solidario',\n",
    "   'cuota_sector_real_comercio', 'cupo_tc_mdo', 'saldo_prom3_tdc_mdo', \n",
    "    'saldo_no_rot_mdo', 'cuota_libranza_sf', 'cant_oblig_tot_sf', 'cant_cast_ult_12m_sr']\n",
    "    \n",
    "    for i in variables_double:\n",
    "    \n",
    "        df[i] = df[i].replace(to_replace = ['foo', '\\\\N', float('nan')], value = [0, 0, 0])\n",
    "        df[i] = [float(i) for i in df[i]]  #convierte todo a flotante\n",
    "    \n",
    "    return df\n",
    "\n",
    "def limpeza_ingresos_gastos(df):\n",
    "    \n",
    "    '''Esta funcion sirve para limpiar e imputar con la mediana las variables ingreso_final , ind y gasto_familiar.\n",
    "    A su vez, elimina ingreso_nomina y ingreso_segurida_social por estar llenos de \\\\N'''\n",
    "    \n",
    "    df = df.drop(columns = ['ingreso_nomina', 'ingreso_segurida_social'])\n",
    "    \n",
    "    variables = ['ingreso_final', 'ind', 'gasto_familiar']\n",
    "    \n",
    "    for i in variables:\n",
    "    \n",
    "        df[i] = df[i].replace(to_replace = ['foo', '\\\\N'], value = [float('nan'), float('nan')])\n",
    "        df[i] = [float(i) for i in df[i]]  #convierte todo a flotante\n",
    "        i_median = df[i].median()  #calcula la mediana \n",
    "        df[i] = df[i].fillna(i_median) \n",
    "    \n",
    "    return df\n",
    "\n",
    "def arreglo_col_mes(df):\n",
    "    \n",
    "    '''Esta funcion simplemente mueve la ultima columna del df (mes) a la primera posicion'''\n",
    "    \n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    df=df[cols]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def Limpieza_calif_cent_ext(df):\n",
    "    \n",
    "    '''En esta imputaremos los valores de \\\\n en el flag de centrales externas como 0 bajo la premisa que siempre\n",
    "    tomamos y por ahora eliminaremos los individuos que no me reporten calificacion (solo son 9%) y no veo como\n",
    "    imputarles un valor valido. Maybe la moda, pero es de discutirse'''\n",
    "    \n",
    "    df.pol_centr_ext = df.pol_centr_ext.replace(to_replace = ['foo', '\\\\N', float('nan')], value = ['0', '0', '0'])\n",
    "    \n",
    "    #df.rep_calif_cred= df.iloc[df.loc[(df.rep_calif_cred!='SIN INFORMACION')&(df.rep_calif_cred!='foo')].index,-4]\n",
    "    moda = df.rep_calif_cred.mode().iloc[0]\n",
    "    df.rep_calif_cred = df.pol_centr_ext.replace(to_replace = ['foo', 'SIN INFORMACION', float('nan')], value = [moda, moda, moda])\n",
    "    df.pol_centr_ext=pd.to_numeric(df.pol_centr_ext)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = open('header.txt', 'r')\n",
    "header = header.read()\n",
    "header = header.split(',')\n",
    "header.append('col1')\n",
    "header.append('col2')\n",
    "\n",
    "dtype_inicial = {i: 'object' for i in header}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_df = dd.read_csv('Dataton_train.csv', names = header, dtype = dtype_inicial) #declaramos el dask dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_df = dask_df.map_partitions(fuck_comas) #implimentamos la correccion del desplazamiento de columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Razones para eliminar variables\n",
    "\n",
    "* id_cli no es necesario conocerlo\n",
    "* fecha_nacimiento para eso se tiene la edad\n",
    "* ciudad_residencia y ciudad_laboral porque son muchas categorias, se prefiere trabajar solo departamento\n",
    "* tenencia_tc porque es un flag que surge de cupo_total_tc\n",
    "* tiene_ctas_activas es redundante con ctas_activas\n",
    "* tiene_ctas_embargadas es redundante con ctas_embargadas\n",
    "* por ahora nos soplamos los departamentos para que cuando agreguemos doomies no sea un chicharron\n",
    "* tambien nos soplamos el CIUU, eso no agrega varianza ni siquiera con la propuesta de reducir categorias\n",
    "* Convenio libranza tampoco aggrega varianza. A pesar de tener mas de 20 valores unicos el 85% de veces es \"\\\\N\" o \"REVISAR CONVENIO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop a priori de variables\n",
    "dask_df = dask_df.drop(columns = ['id_cli', 'fecha_nacimiento', 'ciudad_residencia',\n",
    "                                 'ciudad_laboral', 'tenencia_tc', 'tiene_ctas_activas', \n",
    "                                 'tiene_ctas_embargadas','departamento_residencia', 'departamento_laboral',\n",
    "                                 'codigo_ciiu','convenio_lib'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fin de evitar los foo lo que haremos es aplicar a un df (df0 en este caso) la funcion antes que a las otras particiones y asi en los metadatos de diremos a Dask que clase de output esperar y desaparecen los foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0=dask_df.get_partition(0).compute()\n",
    "df0=t_ult_actual_y_covid(df0)\n",
    "dask_df = dask_df.map_partitions(t_ult_actual_y_covid,meta=df0)\n",
    "df0=edad_to_float(df0)\n",
    "dask_df = dask_df.map_partitions(edad_to_float,meta=df0)\n",
    "df0=genero_limpio(df0)\n",
    "dask_df = dask_df.map_partitions(genero_limpio,meta=df0)\n",
    "df0=estado_civil_limpio(df0)\n",
    "dask_df = dask_df.map_partitions(estado_civil_limpio,meta=df0)\n",
    "df0=nivel_academico_limpio(df0)\n",
    "dask_df = dask_df.map_partitions(nivel_academico_limpio,meta=df0)\n",
    "df0=profesion_limpia(df0)\n",
    "dask_df = dask_df.map_partitions(profesion_limpia,meta=df0)\n",
    "df0=ocupacion_limpia(df0)\n",
    "dask_df = dask_df.map_partitions(ocupacion_limpia,meta=df0)\n",
    "df0=tipo_vivienda_limpia(df0)\n",
    "dask_df = dask_df.map_partitions(tipo_vivienda_limpia,meta=df0)\n",
    "df0=categoria_limpio(df0)\n",
    "dask_df = dask_df.map_partitions(categoria_limpio,meta=df0)\n",
    "df0=ind_binarios_riesgo(df0)\n",
    "dask_df = dask_df.map_partitions(ind_binarios_riesgo,meta=df0)\n",
    "df0=rechazo_credito_limpio(df0)\n",
    "dask_df = dask_df.map_partitions(rechazo_credito_limpio,meta=df0)\n",
    "df0=manejo_de_moras(df0)\n",
    "dask_df = dask_df.map_partitions(manejo_de_moras,meta=df0)\n",
    "df0=limpieza_TdC(df0)\n",
    "dask_df = dask_df.map_partitions(limpieza_TdC,meta=df0)\n",
    "df0=limpieza_obligaciones_cred(df0)\n",
    "dask_df = dask_df.map_partitions(limpieza_obligaciones_cred,meta=df0)\n",
    "df0=limpieza_cuentas(df0)\n",
    "dask_df = dask_df.map_partitions(limpieza_cuentas,meta=df0)\n",
    "df0=limpieza_fopep(df0)\n",
    "dask_df = dask_df.map_partitions(limpieza_fopep,meta=df0)\n",
    "df0=limpieza_hipotecario(df0)\n",
    "dask_df = dask_df.map_partitions(limpieza_hipotecario,meta=df0)\n",
    "df0=limpieza_mediana_nom3(df0)\n",
    "dask_df = dask_df.map_partitions(limpieza_mediana_nom3,meta=df0)\n",
    "df0=limpieza_mediana_pen3(df0)\n",
    "dask_df = dask_df.map_partitions(limpieza_mediana_pen3,meta=df0)\n",
    "df0=limpieza_ingreso_nompen(df0)\n",
    "dask_df = dask_df.map_partitions(limpieza_ingreso_nompen,meta=df0)\n",
    "df0=limpieza_cat_ingreso(df0)\n",
    "dask_df = dask_df.map_partitions(limpieza_cat_ingreso,meta=df0)\n",
    "df0=limpeza_al_estilo_datacredito(df0)\n",
    "dask_df = dask_df.map_partitions(limpeza_al_estilo_datacredito,meta=df0)\n",
    "df0=limpeza_ingresos_gastos(df0)\n",
    "dask_df = dask_df.map_partitions(limpeza_ingresos_gastos,meta=df0)\n",
    "df0=arreglo_col_mes(df0)\n",
    "dask_df = dask_df.map_partitions(arreglo_col_mes,meta=df0)\n",
    "df0=Limpieza_calif_cent_ext(df0)\n",
    "dask_df = dask_df.map_partitions(Limpieza_calif_cent_ext,meta=df0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = dask_df.get_partition(0)\n",
    "df0 = df0.compute()\n",
    "df0.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importante\n",
    "\n",
    "\n",
    "+ las imputaciones se podrian hacer con el MICE https://www.statsmodels.org/stable/generated/statsmodels.imputation.mice.MICEData.html que propuso Ortiz\n",
    "\n",
    "#### Explicacion de donde viene el foo\n",
    "Indeed, if you read the docs for apply, you will see that meta= is a parameter that you can pass, which tells Dask how to expect the output of the operation to look. This is necessary because apply can do very general things.\n",
    "\n",
    "If you don't supply meta=, as in your case, than Dask will try to seed the operation with an example mini-dataframe containing 1 for any numerical columns and \"foo\" for text ones, just to see what the output will be like. Since in your apply you print (and don't actually return anything), you are seeing this seed.\n",
    "\n",
    "As suggested by the documentation, you are always better off providing meta= when possible, and then a whole step in the process can be avoided.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis exploratorio de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogramas(df0, variable):\n",
    "    \n",
    "    '''Esta funcion grafica los histogramas de la variable elegida. Imprime uno con todos los datos y otro\n",
    "    con los datos filtrados para el gasto familiar mayor a cero y menor a 2 millones.\n",
    "    recibe el dataframe de los datos y el string del nombre de la variable'''\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1,figsize=(13, 6))\n",
    "\n",
    "    plt.hist(data = df0, x = variable, density = False, bins = 'doane');\n",
    "    plt.xlabel(variable);\n",
    "    plt.title('Histograma de: ' + variable);\n",
    "\n",
    "    fmt = '{x:,.0f}'\n",
    "    tick = mtick.StrMethodFormatter(fmt)\n",
    "    ax.xaxis.set_major_formatter(tick) \n",
    "\n",
    "    fig, ax = plt.subplots(1, 1,figsize=(13, 6))\n",
    "\n",
    "    plt.hist(data = df0[(df0['gasto_familiar'] > 0) & (df0['gasto_familiar'] < 2000000)], x = variable, density = False, bins = 'doane');\n",
    "    plt.xlabel(variable);\n",
    "    plt.title('Histograma de: ' + variable + \" zoom in entre 0 COP y 2M COP para el gasto familiar\");\n",
    "\n",
    "    fmt = '{x:,.0f}'\n",
    "    tick = mtick.StrMethodFormatter(fmt)\n",
    "    ax.xaxis.set_major_formatter(tick) \n",
    "\n",
    "def percentiles(df0, variable):\n",
    "    \n",
    "    '''Grafica un grafico de pencentiles para la variable seleccionada y hace zoom in para los \n",
    "    que esten entre el percentil 10 y el 90'''\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1,figsize=(8, 5))\n",
    "\n",
    "    percentiles = np.array([0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100])\n",
    "    valores = np.percentile(a = df0[variable], q = percentiles)\n",
    "\n",
    "    plt.plot(percentiles, valores);\n",
    "    plt.xlabel('Percentil');\n",
    "    plt.ylabel(variable);\n",
    "    plt.title(variable + ' por percentiles');\n",
    "\n",
    "    fmt = '{x:,.0f}'\n",
    "    tick = mtick.StrMethodFormatter(fmt)\n",
    "    ax.yaxis.set_major_formatter(tick) \n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1,figsize=(8, 5))\n",
    "\n",
    "    percentiles = np.array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
    "    valores = np.percentile(a = df0[variable], q = percentiles)\n",
    "\n",
    "    plt.plot(percentiles, valores);\n",
    "    plt.xlabel('Percentil');\n",
    "    plt.ylabel(variable);\n",
    "    plt.title(variable + ' por percentiles zoom in entre 10% y 90%');\n",
    "\n",
    "    fmt = '{x:,.0f}'\n",
    "    tick = mtick.StrMethodFormatter(fmt)\n",
    "    ax.yaxis.set_major_formatter(tick) \n",
    "\n",
    "def box_violin(df0, x, y, tipo = 'box'):\n",
    "    \n",
    "    '''Imprime los boxplot o violinplots de la variable 'y' vs el factor 'x'. Tambien imprime los mismo\n",
    "    pero filtrando por gasto familiar entre 0 y 2 millones'''\n",
    "    \n",
    "    if tipo == 'box':\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(13, 5))\n",
    "\n",
    "        sns.boxplot(x, y, data = df0, orient = 'v');\n",
    "        plt.xlabel(x);\n",
    "        plt.ylabel(y);\n",
    "        plt.title('Boxplot por ' + x + ' del ' + y)\n",
    "\n",
    "        fmt = '{x:,.0f}'\n",
    "        tick = mtick.StrMethodFormatter(fmt)\n",
    "        ax.yaxis.set_major_formatter(tick)\n",
    "\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1,figsize=(13, 5))\n",
    "\n",
    "        sns.boxplot(x, y, data = df0[(df0['gasto_familiar'] > 0) & (df0['gasto_familiar'] < 2000000)], orient = 'v');\n",
    "        plt.xlabel(x);\n",
    "        plt.ylabel(y);\n",
    "        plt.title('Boxplot por ' + x + ' del ' + y + ' con zoom in entre 0 COP y 2M COP')\n",
    "\n",
    "        fmt = '{x:,.0f}'\n",
    "        tick = mtick.StrMethodFormatter(fmt)\n",
    "        ax.yaxis.set_major_formatter(tick) \n",
    "        \n",
    "    elif tipo == 'violin':\n",
    "        \n",
    "        fig, ax = plt.subplots(1, 1, figsize=(13, 5))\n",
    "\n",
    "        sns.violinplot(x, y, data = df0, orient = 'v');\n",
    "        plt.xlabel(x);\n",
    "        plt.ylabel(y);\n",
    "        plt.title('Violinplot por ' + x + ' del ' + y)\n",
    "\n",
    "        fmt = '{x:,.0f}'\n",
    "        tick = mtick.StrMethodFormatter(fmt)\n",
    "        ax.yaxis.set_major_formatter(tick)\n",
    "\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1,figsize=(13, 5))\n",
    "\n",
    "        sns.violinplot(x, y, data = df0[(df0['gasto_familiar'] > 0) & (df0['gasto_familiar'] < 2000000)], orient = 'v');\n",
    "        plt.xlabel(x);\n",
    "        plt.ylabel(y);\n",
    "        plt.title('Violinplot por ' + x + ' del ' + y + ' con zoom in entre 0 COP y 2M COP')\n",
    "\n",
    "        fmt = '{x:,.0f}'\n",
    "        tick = mtick.StrMethodFormatter(fmt)\n",
    "        ax.yaxis.set_major_formatter(tick) \n",
    "    \n",
    "def scatter(df0, x, y):   \n",
    "\n",
    "    '''Diagrama de dipersion de la variable y vs la variable x. Obtiene lo mismo para los datos filtrados con \n",
    "    ingreso familiar entre 0 COP y 2 millones. A la y le aplica transformacion logaritmica'''\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1,figsize=(13, 5));\n",
    "    plt.scatter(x, y, data = df0);\n",
    "    plt.xlabel(x);\n",
    "    plt.ylabel(y);\n",
    "    plt.title('Scatterplot de ' + y + ' vs ' + x);\n",
    "    fmt = '{x:,.0f}'\n",
    "    tick = mtick.StrMethodFormatter(fmt)\n",
    "    ax.yaxis.set_major_formatter(tick) \n",
    "    ax.xaxis.set_major_formatter(tick)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(13, 5));\n",
    "    plt.scatter(x, y, data = df0[(df0['gasto_familiar'] > 0) & (df0['gasto_familiar'] < 2000000)]);\n",
    "    plt.xlabel(x);\n",
    "    plt.ylabel(y);\n",
    "    plt.title('Scatterplot de ' + y + ' vs ' + x + ' con zoom in entre 0 COP y 2M COP');\n",
    "    fmt = '{x:,.0f}'\n",
    "    tick = mtick.StrMethodFormatter(fmt)\n",
    "    ax.yaxis.set_major_formatter(tick) \n",
    "    ax.xaxis.set_major_formatter(tick)\n",
    "    \n",
    "def correlacion(df0, x, y):\n",
    "    \n",
    "    '''Calcula tres coeficientes de correlacion para las variables x e y. \n",
    "    Aplica lo mismo filtrando para gasto_familiar entre 0 COP y 2 millones'''\n",
    "    \n",
    "    print('Correlaciones para: ' + x + ' vs ' + y + ':' + '\\n')\n",
    "    \n",
    "    pearson_sin_limpiar = df0[x].corr(df0[y], method = 'pearson') \n",
    "    spearmann_sin_limpiar = df0[x].corr(df0[y], method = 'spearman') \n",
    "    kendall_sin_limpiar = df0[x].corr(df0[y], method = 'kendall') \n",
    "    \n",
    "    print('Pearson sin limpiar: ' + str(pearson_sin_limpiar)[:6])\n",
    "    print('Spearmann sin limpiar: ' + str(spearmann_sin_limpiar)[:6])\n",
    "    print('Kendall sin limpiar: ' + str(kendall_sin_limpiar)[:6] + '\\n')\n",
    "    \n",
    "    df0_filtrado = df0[(df0['gasto_familiar'] > 0) & (df0['gasto_familiar'] < 2000000)]\n",
    "    \n",
    "    pearson_filtrado = df0_filtrado[x].corr(df0_filtrado[y], method = 'pearson') \n",
    "    spearmann_filtrado = df0_filtrado[x].corr(df0_filtrado[y], method = 'spearman') \n",
    "    kendall_filtrado = df0_filtrado[x].corr(df0_filtrado[y], method = 'kendall') \n",
    "    \n",
    "    print('Pearson filtrado: ' + str(pearson_filtrado)[:6])\n",
    "    print('Spearmann filtrado: ' + str(spearmann_filtrado)[:6])\n",
    "    print('Kendall filtrado: ' + str(kendall_filtrado)[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogramas(df0, 'gasto_familiar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles(df0, 'gasto_familiar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "box_violin(df0, 'rep_calif_cred', 'gasto_familiar', 'box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scatter(df0, 'ind', 'gasto_familiar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlacion(df0, 'ind', 'gasto_familiar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalización del preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con las funciones anteriores, se analizó variable por variable analizando si agregaba o no agregaba varianza. De manera que se pudiese elegir cuales variables merecen ser eliminadas y cuales tienen el beneficio de la deuda.\n",
    "\n",
    "Ahora se retiran las variables que no afectan la variable de respuesta. Revisar word con el nombre 'limpieza a partir del eda.docx' para verla categorizacion de las variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop con el EDA de variables\n",
    "dask_df = dask_df.drop(columns = ['mes', 'covid', 'edad', 'genero', \n",
    "                                 't_ult_actual', 'ind_mora_vigente', 'cartera_castigada', \n",
    "                                 'cupo_total_tc', 'cuota_tc_bancolombia', 'tiene_consumo', \n",
    "                                 'pension_fopep', 'mediana_nom3', 'mediana_pen3', \n",
    "                                'ingreso_nompen','cuota_de_consumo', 'cuota_rotativos',\n",
    "                                  'cuota_tarjeta_de_credito', 'cuota_de_sector_solidario',\n",
    "                                  'cuota_sector_real_comercio', 'saldo_prom3_tdc_mdo',\n",
    "                                  'saldo_no_rot_mdo', 'cuota_libranza_sf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = dask_df.get_partition(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = df0.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se convierten a numerico las variables que no quedaron como tal en el preprocesamiento anterior y se obtienen las dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalizar_preprocesamiento(df0):\n",
    "    \n",
    "    '''Funcion que obtiene los dummies, transforma a numerico las que no quedaron como tal y \n",
    "    lleva la variable gasto al final del dataframe'''\n",
    "    \n",
    "    \n",
    "    #Obtener dummies\n",
    "    list_dummies = ['estado_civil', 'nivel_academico', 'ocupacion', 'tipo_vivienda', 'cat_ingreso']\n",
    "    \n",
    "    for i in list_dummies:\n",
    "        var_dummies = pd.get_dummies(df0[i])\n",
    "        df0.drop(columns = [i], inplace = True)\n",
    "        df0 = pd.concat([df0, var_dummies], axis = 1)\n",
    "        \n",
    "    #Transformar a numerico\n",
    "    \n",
    "    #rep_calif_cred es cuestionable convertirla a numerico. Hay que debatirlo\n",
    "    to_numericas = ['categoria', 'tiene_cred_hipo_1', 'tiene_cred_hipo_2', 'rep_calif_cred']\n",
    "    \n",
    "    for i in to_numericas:\n",
    "        #df0[i] = df0[i].replace(to_replace = ['foo'], value = [float('nan')])\n",
    "        df0[i] = [float(j) for j in df0[i]]\n",
    "    \n",
    "    \n",
    "    #llevar gasto_familiar al final\n",
    "    cols = df0.columns.tolist()\n",
    "    cols = cols[-36:] + cols[:-36]\n",
    "    df0 = df0[cols]\n",
    "    \n",
    "    return df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = dask_df.get_partition(0)\n",
    "df0 = df0.compute()\n",
    "df0 = finalizar_preprocesamiento(df0)\n",
    "dask_df = dask_df.map_partitions(finalizar_preprocesamiento,meta=df0) #el foo cagaba todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aca volvemos a sacar el Df0 del dask df para comprobar que todo aplico bien\n",
    "df0 = dask_df.get_partition(0)\n",
    "df0 = df0.compute()\n",
    "df0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtener los registros que se deben predecir y otros concernientes a la finalización del preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente desarrollo permite obtener los registros que se deben predecir, ya preprocesados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_X_test():  \n",
    "    \n",
    "    '''La siguiente funcion preprocesa el data set a predecir y devuelve una tupa con X_test y otra con id_registro'''\n",
    "    \n",
    "    header = open('header.txt', 'r')\n",
    "    header = header.read()\n",
    "    header = header.split(',')\n",
    "    header.append('col1')\n",
    "    header.append('col2')\n",
    "\n",
    "    dtype_inicial = {i: 'object' for i in header}\n",
    "\n",
    "    header.insert(0, 'id_registro')\n",
    "\n",
    "    bd_test = dd.read_csv('dt2020_base_evaluar.csv', names = header, dtype = dtype_inicial)\n",
    "\n",
    "    bd_test = bd_test.map_partitions(fuck_comas) #implimentamos la correccion del desplazamiento de columnas\n",
    "\n",
    "    bd_test = bd_test.map_partitions(t_ult_actual_y_covid)\n",
    "    bd_test = bd_test.map_partitions(edad_to_float) \n",
    "    bd_test = bd_test.map_partitions(genero_limpio)\n",
    "    bd_test = bd_test.map_partitions(estado_civil_limpio)\n",
    "    bd_test = bd_test.map_partitions(nivel_academico_limpio)\n",
    "    bd_test = bd_test.map_partitions(profesion_limpia)\n",
    "    bd_test = bd_test.map_partitions(ocupacion_limpia)\n",
    "    bd_test = bd_test.map_partitions(tipo_vivienda_limpia)\n",
    "    bd_test = bd_test.map_partitions(categoria_limpio)\n",
    "    bd_test = bd_test.map_partitions(ind_binarios_riesgo)\n",
    "    bd_test = bd_test.map_partitions(rechazo_credito_limpio)\n",
    "    bd_test = bd_test.map_partitions(manejo_de_moras)\n",
    "    bd_test = bd_test.map_partitions(limpieza_TdC)\n",
    "    bd_test = bd_test.map_partitions(limpieza_obligaciones_cred)\n",
    "    bd_test = bd_test.map_partitions(limpieza_cuentas)\n",
    "    bd_test = bd_test.map_partitions(limpieza_fopep)\n",
    "    bd_test = bd_test.map_partitions(limpieza_hipotecario)\n",
    "    bd_test = bd_test.map_partitions(limpieza_mediana_nom3)\n",
    "    bd_test = bd_test.map_partitions(limpieza_mediana_pen3)\n",
    "    bd_test = bd_test.map_partitions(limpieza_ingreso_nompen)\n",
    "    bd_test = bd_test.map_partitions(limpieza_cat_ingreso)\n",
    "    bd_test = bd_test.map_partitions(limpeza_al_estilo_datacredito)\n",
    "    bd_test = bd_test.map_partitions(limpeza_ingresos_gastos)\n",
    "    bd_test = bd_test.map_partitions(arreglo_col_mes)\n",
    "    bd_test = bd_test.map_partitions(Limpieza_calif_cent_ext)\n",
    "\n",
    "    bd_test = bd_test.drop(columns = ['id_cli', 'fecha_nacimiento', 'ciudad_residencia',\n",
    "                                     'ciudad_laboral', 'tenencia_tc', 'tiene_ctas_activas', \n",
    "                                     'tiene_ctas_embargadas','departamento_residencia', 'departamento_laboral',\n",
    "                                     'codigo_ciiu','convenio_lib'])\n",
    "\n",
    "    bd_test = bd_test.drop(columns = ['mes', 'covid', 'edad', 'genero', \n",
    "                                     't_ult_actual', 'ind_mora_vigente', 'cartera_castigada', \n",
    "                                     'cupo_total_tc', 'cuota_tc_bancolombia', 'tiene_consumo', \n",
    "                                     'pension_fopep', 'mediana_nom3', 'mediana_pen3', \n",
    "                                    'ingreso_nompen','cuota_de_consumo', 'cuota_rotativos',\n",
    "                                      'cuota_tarjeta_de_credito', 'cuota_de_sector_solidario',\n",
    "                                      'cuota_sector_real_comercio', 'saldo_prom3_tdc_mdo',\n",
    "                                      'saldo_no_rot_mdo', 'cuota_libranza_sf'])\n",
    "\n",
    "    bd_test_0 = bd_test.get_partition(0)\n",
    "    bd_test_0 = bd_test_0.compute()\n",
    "    #bd_test_0.head()\n",
    "\n",
    "    bd_test_1 = bd_test.get_partition(1)\n",
    "    bd_test_1 = bd_test_1.compute()\n",
    "    #bd_test_1.head()\n",
    "\n",
    "    bd_test_2 = bd_test.get_partition(2)\n",
    "    bd_test_2 = bd_test_2.compute()\n",
    "    #bd_test_2.head()\n",
    "\n",
    "    X_test = pd.concat([bd_test_0, bd_test_1, bd_test_2], axis = 0)\n",
    "    X_test.drop(columns = ['gasto_familiar'], inplace = True)\n",
    "    id_registro = X_test['id_registro']\n",
    "    X_test.drop(columns = ['id_registro'], inplace = True)\n",
    "\n",
    "    #Obtener dummies\n",
    "    list_dummies = ['estado_civil', 'nivel_academico', 'ocupacion', 'tipo_vivienda', 'cat_ingreso']\n",
    "\n",
    "    for i in list_dummies:\n",
    "        var_dummies = pd.get_dummies(X_test[i])\n",
    "        X_test.drop(columns = [i], inplace = True)\n",
    "        X_test = pd.concat([X_test, var_dummies], axis = 1)\n",
    "\n",
    "    #Transformar a numerico\n",
    "\n",
    "    #rep_calif_cred es cuestionable convertirla a numerico. Hay que debatirlo\n",
    "    to_numericas = ['categoria', 'tiene_cred_hipo_1', 'tiene_cred_hipo_2', 'rep_calif_cred']\n",
    "\n",
    "    for i in to_numericas:\n",
    "        #df0[i] = df0[i].replace(to_replace = ['foo'], value = [float('nan')])\n",
    "        X_test[i] = [float(j) for j in X_test[i]]\n",
    "\n",
    "    #llevar gasto_familiar al final\n",
    "    cols = X_test.columns.tolist()\n",
    "    cols = cols[-36:] + cols[:-36]\n",
    "    X_test = X_test[cols]\n",
    "\n",
    "    return (X_test, id_registro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, id_registro = obtener_X_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_registro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente funcion es para imprimir los resultados en formato csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imprimir_resultado(y_test_pred, nombre_archivo):\n",
    "    \n",
    "    '''Funcion que imprime el resultado, al nombre del archivo debe agregarse un .csv'''\n",
    "    \n",
    "    data = {'id_registro': id_registro, 'gasto_familiar': y_test_pred}\n",
    "    \n",
    "    output = pd.DataFrame(data)\n",
    "    output = output.set_index('id_registro')\n",
    "    output.to_csv(nombre_archivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente funcion es para obtener las primeras ### particiones del dask, pronto será deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def primeras_particiones(k, atipicos):\n",
    "    \n",
    "    '''Entrega una tupla con X e y del dask, indicando las primeras k particiones a tomar en cuenta. \n",
    "    Entrega el resultado filtrado con abs(Z) > 2.7 or gastos_familiar_{i} negativos si atipicos = 'Z' o solo \n",
    "    lo de los negativos si atipicos = 'neg'. \n",
    "    Siendo Z = (xi - med)/mad'''\n",
    "    \n",
    "    part_7dddf = dask_df.get_partition(0).compute()\n",
    "    \n",
    "    for i in range(1, k):\n",
    "        part_7dddf = pd.concat([part_7dddf,dask_df.get_partition(i).compute()], axis = 0)\n",
    "        \n",
    "    X = part_7dddf.iloc[:, 0:60] \n",
    "    y = part_7dddf[\"gasto_familiar\"]\n",
    "    \n",
    "    med = part_7dddf['gasto_familiar'].median()\n",
    "    mad = median_absolute_deviation(part_7dddf['gasto_familiar'])\n",
    "    \n",
    "    if atipicos == 'Z':\n",
    "    \n",
    "        indicador = [False if (abs(i-med)/mad) > 2.7 or i < 0 else True for i in part_7dddf['gasto_familiar']]\n",
    "        \n",
    "    elif atipicos == 'neg':\n",
    "        \n",
    "        indicador = [False if i < 0 else True for i in part_7dddf['gasto_familiar']]\n",
    "        \n",
    "    \n",
    "    part_7dddf = part_7dddf.iloc[indicador]\n",
    "    X = X.iloc[indicador] \n",
    "    y = y.iloc[indicador]\n",
    "    \n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = primeras_particiones(20, atipicos = 'neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente funcion calcula el MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducción de dimensiones, cluster y clasificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código aplica PCA a los datasets de X y X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#se declara el escalador\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#se entrena el escalador\n",
    "scaler.fit(X)\n",
    "\n",
    "#se escalan los datos\n",
    "X_scaled = scaler.transform(X)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#se declara el PCA\n",
    "pca = PCA(.95)\n",
    "\n",
    "#se entrena el PCA\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "#se transforman los datos escalados a PCA's\n",
    "X_pca = pca.transform(X_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "#se renombran las variables\n",
    "\n",
    "X = X_pca\n",
    "X_test = X_test_pca\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente clustering para los datos, notese que no funciono, como lo muestran las graficas de inercia. Primero se hizo hasta k=20 y luego hasta k=40, pero si se hace para más, el codo seguira siendo impercentible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#esta es la inercia para un kmeans corrido desde k=2 hasta k=39\n",
    "\n",
    "#for i in range (20, 40):\n",
    "#    kmeans = KMeans(n_clusters = i, random_state = 0).fit(X)\n",
    "#    inertia.append(kmeans.inertia_)\n",
    "\n",
    "inertia = [77112762.79291268, 74331591.13173904, 71957295.60106312, 70071725.40839572, 66745896.11933103, 65267013.742731795,\n",
    "           63789473.7435838, 62288995.86606553, 60501387.813032605, 59038854.92246048, 57651711.06445355, 56340192.00861995,\n",
    "           54955176.61438507, 53557538.35161901, 52610487.46659651, 51214593.92215118, 49845423.85100328, 48777525.3523114, \n",
    "           47960946.72825996, 46191257.500313915, 45725034.43820399, 44757639.20487177, 43892272.63960439, 42619398.43980851, \n",
    "           42071740.27109539, 41179808.436337605, 39862378.28405561, 38798079.69976133, 38398742.84839752, 37679648.0023016, \n",
    "           36659210.332658805, 35872029.634098604, 35347221.73675595,34958985.8747368, 34538836.417099416, 33908841.54218117, \n",
    "           33375833.292873614, 32743859.728969246]\n",
    "\n",
    "plt.plot(range(2, 40), inertia);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([(inertia[i-1] - inertia[i])*100/inertia[i-1] for i in range(1, len(inertia))]).plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ahora aca si estamos haciendo bien la regla del codo para clustering\n",
    "\n",
    "XD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "model = KMeans()\n",
    "# k is range of number of clusters.\n",
    "visualizer = KElbowVisualizer(model, k=(2,30), timings= True)\n",
    "visualizer.fit(X)        # Fit data to visualizer\n",
    "visualizer.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = KElbowVisualizer(model, k=(30,60), timings= True)\n",
    "visualizer.fit(X)        # Fit data to visualizer\n",
    "visualizer.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = KElbowVisualizer(model, k=(60,80), timings= True)\n",
    "visualizer.fit(X)        # Fit data to visualizer\n",
    "visualizer.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aqui comienza la clusterizacion ideal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 46, random_state = 0).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistica = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n",
    "logistica.fit(X, kmeans.labels_)\n",
    "logistica.score(X, kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(pd.Series(kmeans.labels_).unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que funcione mi logica necesito X y Y como dataframes de nuevo asi que los convierto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.DataFrame(X)\n",
    "y=pd.DataFrame(y)\n",
    "X[\"label\"]=kmeans.labels_\n",
    "y[\"label\"]=kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despues de entrenar la hija de puta logistica tanto tiempo vamos a entrenar todas las ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridges=[Ridge(alpha = 0.00001, normalize=True) for i in range(46)]\n",
    "for i in range(46):\n",
    "    ridges[i].fit(X.iloc[X.loc[X[\"label\"]==i].index,:-1],y.iloc[y.loc[y[\"label\"]==i].index,:-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora clasificaremos los datos de test y le sacamos de una vez las predicciones en orden.\n",
    "Debe haber una mejor manera de sacarla que no involucre la list comprehension, pero no se me ocurrió como para\n",
    "que siguiera en orden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test=logistica.predict(X_test)\n",
    "X_test=pd.DataFrame(X_test)\n",
    "X_test['label']=labels_test\n",
    "por el metodo iloc toca darle una forma optima para que la regresion pueda leer bien los datos\n",
    "# aca nos figuro darle la forma apropiada a cada set de datos de x para que el predict lo leyera\n",
    "y_test_pred=[ridges[X_test.iloc[i,-1]].predict(np.asarray(X_test.iloc[i,:-1]).reshape(1, 43)) for i in range(len(X_test))]\n",
    "# aqui lo volvemos una lista de integers y no de arrays\n",
    "y_test_pred=[item for i in y_test_pred for item in i.tolist ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imprimir_resultado(y_test_pred,\"The Data Machinery Submission 28.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificador de ricos, medios y pobres con Reg Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dejemos a un lado el enfoque de cluster y pasemos al de clasificacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "P95 = np.percentile(a = y, q = np.array([95]))\n",
    "etiqueta = ['Rico' if i >= P95 else 'No Rico' for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistica = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistica.fit(X, etiqueta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9455631948019545"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistica.score(X, etiqueta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La logística anterior predice muy bien, aparentemente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ricos = [True if i == 'Rico' else False for i in etiqueta]  \n",
    "No_ricos = [True if i == 'No Rico' else False for i in etiqueta]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ricos = pd.DataFrame(X).iloc[Ricos]\n",
    "y_ricos = pd.DataFrame(y).iloc[Ricos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_no_ricos = pd.DataFrame(X).iloc[No_ricos]\n",
    "y_no_ricos = pd.DataFrame(y).iloc[No_ricos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142285"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_ricos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2703400"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_no_ricos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_ricos = Ridge(alpha = 0.00001, normalize=True)\n",
    "ridge_no_ricos = Ridge(alpha = 0.00001, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1e-05, normalize=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_ricos.fit(X_ricos, y_ricos)\n",
    "ridge_no_ricos.fit(X_no_ricos, y_no_ricos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mape de la ridge no ricos: 41.362660711146084\n"
     ]
    }
   ],
   "source": [
    "print(\"Mape de la ridge no ricos: \" + str(mean_absolute_percentage_error(np.array(y_no_ricos) + 1000, ridge_no_ricos.predict(X_no_ricos))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "for i in range(10, 90, 5):\n",
    "    \n",
    "    Pi = np.percentile(a = y_no_ricos.gasto_familiar, q = np.array([i]))\n",
    "    etiqueta_2 = ['Medio' if i>=Pi else 'Pobre' for i in y_no_ricos.gasto_familiar]\n",
    "    \n",
    "    logistica_2 = LogisticRegression()\n",
    "    logistica_2.fit(X_no_ricos, etiqueta_2)\n",
    "    scores.append(logistica_2.score(X_no_ricos, etiqueta_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scores).plot();\n",
    "plt.vlines(2, ymin = 0.75, ymax = 0.85);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "P20 = np.percentile(a = y_no_ricos.gasto_familiar, q = np.array([20]))\n",
    "etiqueta_2 = ['Medio' if i>=P20 else 'Pobre' for i in y_no_ricos.gasto_familiar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7999178811866539"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistica_2 = LogisticRegression()\n",
    "logistica_2.fit(X_no_ricos, etiqueta_2)\n",
    "logistica_2.score(X_no_ricos, etiqueta_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Medio = [True if i == 'Medio' else False for i in etiqueta_2]  \n",
    "Pobre = [True if i == 'Pobre' else False for i in etiqueta_2]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_medio = pd.DataFrame(X_no_ricos).iloc[Medio]\n",
    "y_medio = pd.DataFrame(y_no_ricos).iloc[Medio]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pobre = pd.DataFrame(X_no_ricos).iloc[Pobre]\n",
    "y_pobre = pd.DataFrame(y_no_ricos).iloc[Pobre]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2162720"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_medio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "540680"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pobre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.674254642302286"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nulidad_pobre=[\"nulo\" if i == 0 else \"no_nulo\" for i in y_pobre.gasto_familiar]\n",
    "Logistica_de_pobres=LogisticRegression()\n",
    "Logistica_de_pobres.fit(X_pobre,nulidad_pobre)\n",
    "Logistica_de_pobres.score(X_pobre,nulidad_pobre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no_nulo    256868\n",
       "nulo        24798\n",
       "dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nulidad_pobre_pred=Logistica_de_pobres.predict(X_test)\n",
    "X_pobre['nulidad']=nulidad_pobre\n",
    "pd.Series(nulidad_pobre_pred).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_medio = Ridge(alpha = 0.00001, normalize = True)\n",
    "ridge_pobre = Ridge(alpha = 0.00001, normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1e-05, normalize=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_medio.fit(X_medio, y_medio)\n",
    "ridge_pobre.fit(X_pobre.iloc[X_pobre.loc[X_pobre.nulidad==\"no_nulo\"].index,:-1], y_pobre.iloc[X_pobre.loc[X_pobre.nulidad==\"no_nulo\"].index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mape de la ridge medios: 11.998408017745597\n"
     ]
    }
   ],
   "source": [
    "print(\"Mape de la ridge medios: \" + str(mean_absolute_percentage_error(np.array(y_medio) , ridge_ricos.predict(X_medio))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mape de la ridge pobres: 1878.868067083032\n"
     ]
    }
   ],
   "source": [
    "print(\"Mape de la ridge pobres: \" + str(mean_absolute_percentage_error(np.array(y_pobre.iloc[X_pobre.loc[X_pobre.nulidad==\"no_nulo\"].index]) + 1000, ridge_ricos.predict(X_pobre.iloc[X_pobre.loc[X_pobre.nulidad==\"no_nulo\"].index,:-1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "etiqueta1_pred=logistica.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Medio    281309\n",
       "dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_no_ricos=[True if i=='No Rico' else False for i in etiqueta1_pred]\n",
    "X_test_no_ricos=X_test.iloc[ind_no_ricos,:]\n",
    "etiqueta2_pred=logistica_2.predict(X_test_no_ricos)\n",
    "pd.Series(etiqueta2_pred).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1=ridge_ricos.predict(X_test)\n",
    "y_pred_1[ind_no_ricos]=ridge_medio.predict(X_test_no_ricos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1[ind_no_ricos]=0.5*y_pred_1[ind_no_ricos]\n",
    "y_pred_1 = [item for i in y_pred_1 for item in i.tolist()]\n",
    "imprimir_resultado(y_pred_1,\"The Data Machinery submission 39.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [item for i in y_pred_1 for item in i.tolist()]\n",
    "imprimir_resultado(y_pred,\"The Data Machinery submission 37.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogramas(y_pobre, 'gasto_familiar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles(y_no_ricos, 'gasto_familiar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece que no hay diferencias entre las variables diferentes a gasto_familiar entre las personas más pobres. Las cuales por cierto, tienen su gatso_familiar en cero. Haremos una regresion que no incluya a los de gasto familiar = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(y)\n",
    "X = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.iloc[y.loc[y.gasto_familiar != 0].index]\n",
    "y = y.iloc[y.loc[y.gasto_familiar != 0].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_sin_pobres_weon = Ridge(alpha = 0.00001, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_sin_pobres_weon.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mape de la ridge completa sin ceros: \" + str(mean_absolute_percentage_error(np.array(y) + 1000, ridge_sin_pobres_weon.predict(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ridge_sin_pobres_weon.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [item for i in y_pred for item in i.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imprimir_resultado(y_pred, 'The Data Machinery Submission 29.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aca intentaremos clasificar a los raros que dan 0 en vez de meterlos a regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etiqueta = ['nulo' if i == 0 else 'no_nulo' for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(pred_0).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistica_0=LogisticRegression()\n",
    "#logistica_0.fit(X,etiqueta)\n",
    "#logistica_0.score(X,etiqueta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_0=logistica_0.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"nulidad\"]=etiqueta\n",
    "ridge_no_nulos=Ridge(alpha = 0.00001, normalize=True)\n",
    "ridge_no_nulos.fit(X.iloc[X.loc[X.nulidad==\"no_nulo\"].index,:-1],y.iloc[X.loc[X.nulidad==\"no_nulo\"].index,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[\"nulidad\"]=pred_0\n",
    "X_test.iloc[X_test.loc[X_test.nulidad==\"no_nulo\"].index,:-1]\n",
    "y_test=ridge_no_nulos.predict(X_test.iloc[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=ridge_medio.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [item for i in y_pred for item in i.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imprimir_resultado(y_pred,\"The Data Machinery Submission 35.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Con X_ricos y X_no_ricos logré dos Ridge que tienen 0.5% y 40% de MAPE puede implementarse otra oarticion a los no_ricos para sacar medios y pobresy asi entrenar tres regresiones. pipe haz lo tuyo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pendientes pre modelamiento\n",
    "\n",
    "+ la transformacion de rep_calif_cred de object a float es muy debatible, hay que revisarlo\n",
    "\n",
    "+ integrar R para probar el kurtosis projections\n",
    "\n",
    "+ esperar resultados de Ortiz al ejecutar en R el codigo\n",
    "\n",
    "+ Implementar PCA's para disminuir dimensiones y así mejorar la velocidad de ajuste de los modelos https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajuste de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelamiento breve con Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestRegressor(n_estimators = 6, max_depth = 2,  random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mape_train_general: \" + str(mean_absolute_percentage_error(np.array(y) + 1000, random_forest.predict(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imprimir_resultado(random_forest.predict(X_test), 'ensayo mil con random forest.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelamiento super breve con RIDGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ridge = Ridge(alpha = 0.00001, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ridge.fit(X_no_ricos, y_no_ricos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mape_train_general: \" + str(mean_absolute_percentage_error(np.array(y_no_ricos) + 1000, reg_ridge.predict(X_no_ricos))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imprimir_resultado(reg_ridge.predict(X_test), 'The Data Machinery Submission 27.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelamiento breve con SVM Regressor ese hpta es muy lento, lo corre su madre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svr_model = SVR(kernel = 'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svr_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Mape_train_general: \" + str(mean_absolute_percentage_error(np.array(y)+1000, svr_model.predict(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test_pred = svr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelamiento breve con KNN Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_regressor = KNeighborsRegressor(n_neighbors = 300)\n",
    "knn_regressor.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mape_train_general: \" + str(mean_absolute_percentage_error(np.array(y)+1000, knn_regressor.predict(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = knn_regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imprimir_resultado(y_test_pred, 'ensayo tres mil con KNN.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelamiento breve con Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_boosting = GradientBoostingRegressor(n_estimators = 10, max_depth = 3, learning_rate = 0.1, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_boosting.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mape_train_general: \" + str(mean_absolute_percentage_error(np.array(y)+1000, gradient_boosting.predict(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio de Paralelismo en métodos de predicción\n",
    "\n",
    "Aca abajo intentamso corre un gradient boosting de dask con xgboost pero nos cagó el tener variables categoricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.distributed\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'id_registro':id_registro, 'gasto_familiar':y_test_pred}\n",
    "output = pd.DataFrame(data)\n",
    "output = output.set_index('id_registro')\n",
    "output.to_csv('The Data Machinery Submission 10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster = dask.distributed.LocalCluster(n_workers=8, threads_per_worker=4)\n",
    "client = dask.distributed.Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dask_df.iloc[:, 0:60] \n",
    "y = dask_df[\"gasto_familiar\"] #la variable a predecir\n",
    "#dtrain = xgb.dask.DaskDMatrix(client, X, y,enable_categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = xgb.dask.train(client,\n",
    "                        {'verbosity': 2,\n",
    "                         'tree_method': 'hist',\n",
    "                         'objective': 'reg:squarederror'\n",
    "                         },\n",
    "                        dtrain,\n",
    "                        num_boost_round=4, evals=[(dtrain, 'train')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aca intentamos correr un randomforest con muchos umpaloompas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#client = Client(processes=False)             # create local cluster\n",
    "# client = Client(\"scheduler-address:8786\")  # or connect to remote cluster\n",
    "random_forest = RandomForestRegressor(n_estimators = 150, max_depth = 5,  random_state = 0)\n",
    "with joblib.parallel_backend('dask'):\n",
    "    random_forest.fit(X, y)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 587,
   "position": {
    "height": "40px",
    "left": "12px",
    "right": "20px",
    "top": "4px",
    "width": "337px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
